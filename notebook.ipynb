{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "plt.style.use([\"ggplot\", \"seaborn-poster\"])\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeding everything for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS      = 5\n",
    "SEED       = 42\n",
    "DISPLAY    = True\n",
    "ROUND      = lambda x: np.round(x, 4)\n",
    "STORE_DATA = False\n",
    "\n",
    "PATH_TO_TRAIN_DATA  = \"data/Lyrics-Genre-Train.csv\"\n",
    "PATH_TO_TEST_DATA   = \"data/Lyrics-Genre-Test-GroundTruth.csv\"\n",
    "PATH_TO_STORAGE     = \"storage/\" \n",
    "PATH_TO_PREDICTIONS = 'predictions/'\n",
    "\n",
    "seed_everything(seed = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "- For this project we are not allowed to use other features besides the lyrics, so we will remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(PATH_TO_TRAIN_DATA) \n",
    "test_data  = pd.read_csv(PATH_TO_TEST_DATA)\n",
    "\n",
    "train_data.drop([\"Song\", \"Song year\", \"Artist\"], axis = 1, inplace = True)\n",
    "test_data.drop([\"Song\", \"Song year\", \"Artist\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Metal</td>\n",
       "      <td>I am a night in to the darkness, only soul los...</td>\n",
       "      <td>18096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>Yeah\\nSometimes, i just wanna fly away.\\nThey ...</td>\n",
       "      <td>22724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metal</td>\n",
       "      <td>Do you work hard?\\nDo you work hard?\\nYou don'...</td>\n",
       "      <td>24760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Genre                                             Lyrics  Track_id\n",
       "0    Metal  I am a night in to the darkness, only soul los...     18096\n",
       "1  Hip-Hop  Yeah\\nSometimes, i just wanna fly away.\\nThey ...     22724\n",
       "2    Metal  Do you work hard?\\nDo you work hard?\\nYou don'...     24760"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(18513, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>Most folks spend their days daydreaming of fin...</td>\n",
       "      <td>8294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indie</td>\n",
       "      <td>Take your cold hands and put them on my face\\n...</td>\n",
       "      <td>21621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metal</td>\n",
       "      <td>Are you ready it's time for war\\nWe'll break d...</td>\n",
       "      <td>3301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Genre                                             Lyrics  Track_id\n",
       "0  Hip-Hop  Most folks spend their days daydreaming of fin...      8294\n",
       "1    Indie  Take your cold hands and put them on my face\\n...     21621\n",
       "2    Metal  Are you ready it's time for war\\nWe'll break d...      3301"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(7935, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if DISPLAY:\n",
    "    display(train_data.head(n = 3))\n",
    "    display(train_data.shape)\n",
    "\n",
    "    display(test_data.head(n = 3))\n",
    "    display(test_data.shape)\n",
    "\n",
    "    plt.figure(figsize = (18, 6))\n",
    "    plt.title(\"Train Label Distribution\", size = 14, color = 'red')\n",
    "    plt.bar(*np.unique(train_data['Genre'], return_counts = True))\n",
    "    plt.tick_params(axis = 'both', colors = 'red')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (18, 6))\n",
    "    plt.title(\"Test Label Distribution\", size = 14, color = 'red')\n",
    "    plt.bar(*np.unique(test_data['Genre'], return_counts = True))\n",
    "    plt.tick_params(axis = 'both', colors = 'red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "encoder.fit(train_data['Genre'].values.reshape(-1, 1))\n",
    "train_data['Genre'] = encoder.transform(train_data['Genre'].values.reshape(-1, 1)).astype(np.uint8)\n",
    "test_data['Genre']  = encoder.transform(test_data['Genre'].values.reshape(-1, 1)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the folds from the beginning to ensure no data leakage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train_data, train_data['Genre'].values)):\n",
    "    train_data.loc[valid_idx, 'Fold'] = fold\n",
    "\n",
    "train_data['Fold'] = train_data['Fold'].astype(int)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main idea of this project\n",
    "## Combined Model based on statistic structure, semantic structure and lyrics similarity encoding\n",
    "- To obtain a broad perspective about this problem we need to look at it from various stand-points\n",
    "### 1. Statistic Structure: \n",
    "Extracting statistical features based on Term Frequency - Inverse Documnet Frequency at word and character level for different n-grams\n",
    "### 2. Semantic Structure:\n",
    "To be able able to extract and quantify various semantinc relations from the lyrics we will try to use state of the art transformer models\n",
    "### 3. Lyrics Similarity\n",
    "For lyrics similarity we will use a pretrained transformer model for extracting embeddings to be later used for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Creating statistical features with TF-IDF and storing them\n",
    "- To be sure that we don't waste time recreating steps we will store the created features\n",
    "\n",
    "### For retaining as much style our experiments during this project will be done on raw data\n",
    "- The reason for that is because even basic cleaning steps as e.g. stopwords removal can affect the predictability of the genre based on lyrics\n",
    "- To be researched proper cleaning techniques for text cleaning and preprocessing in future experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STORE_DATA:\n",
    "    NGRAM_RANGES = [(1, 1), (1, 2), (1, 3), (1, 5), (1, 7)]\n",
    "    ANALYZERS    = [\"word\", \"char\", \"char_wb\"]\n",
    "\n",
    "    for fold in range(FOLDS):\n",
    "        if os.path.isdir(os.path.join(PATH_TO_STORAGE, f'fold-{fold}')) == False: \n",
    "            os.makedirs(os.path.join(PATH_TO_STORAGE, f'fold-{fold}'))\n",
    "\n",
    "        X_train_fold = train_data[train_data[\"Fold\"] != fold]\n",
    "        X_valid_fold = train_data[train_data[\"Fold\"] == fold]\n",
    "\n",
    "        train_features = X_train_fold[\"Lyrics\"]\n",
    "        valid_features = X_valid_fold[\"Lyrics\"]\n",
    "        test_features  = test_data[\"Lyrics\"]\n",
    "        \n",
    "        for n_grams in NGRAM_RANGES:\n",
    "            for analyzer in ANALYZERS:\n",
    "                tfidf = TfidfVectorizer(ngram_range = n_grams, analyzer = analyzer)\n",
    "                tfidf.fit(train_features)\n",
    "\n",
    "                X_train_features = tfidf.transform(train_features)\n",
    "                X_valid_features = tfidf.transform(valid_features)\n",
    "                X_test_features  = tfidf.transform(test_features)\n",
    "                \n",
    "                scipy.sparse.save_npz(\n",
    "                    os.path.join(PATH_TO_STORAGE, f'fold-{fold}', f'train_features_ngrams_{n_grams}_analyzer_{analyzer}.npz'), \n",
    "                    X_train_features\n",
    "                )\n",
    "\n",
    "                scipy.sparse.save_npz(\n",
    "                    os.path.join(PATH_TO_STORAGE, f'fold-{fold}', f'valid_features_ngrams_{n_grams}_analyzer_{analyzer}.npz'), \n",
    "                    X_valid_features\n",
    "                )\n",
    "                \n",
    "                scipy.sparse.save_npz(\n",
    "                    os.path.join(PATH_TO_STORAGE, f'fold-{fold}', f'test_features_ngrams_{n_grams}_analyzer_{analyzer}.npz'), \n",
    "                    X_test_features\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the best models with regards of context\n",
    "- To be able to choose the best preprocessing context that increases the predictability of the genre based on lyrics we will let our models to decide which of those is most suitable for our problem\n",
    "- Based on this approach we will also increase the variance in the ensamble, some models will see the problem from the word perspective, other from character perspective\n",
    "- This should lead to less correlated predictions and a higher accuracy\n",
    "\n",
    "### At this stage, because of time constraints, we will experiment only on the first fold\n",
    "### Why this is not a problem?\n",
    "- Our dataset is stratified based on genre, so we should expect that our experimets to be similar with those on the other folds and our cross-validation metric should not differ by much from the initial results\n",
    "- To be researched for better stratification methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NGRAM_RANGES = [(1, 1), (1, 2), (1, 3), (1, 5), (1, 7)]\n",
    "ANALYZERS    = [\"word\", \"char_wb\"]\n",
    "\n",
    "FOLD   = 0\n",
    "MODELS = [\n",
    "    {\n",
    "        \"id\": \"LGBM-Classifier-1\",\n",
    "        \"model\": LGBMClassifier, \n",
    "        \"params\": {\"n_estimators\": 30, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\": \"LGBM-Classifier-2\",\n",
    "        \"model\": LGBMClassifier, \n",
    "        \"params\": {\"n_estimators\": 50, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\": \"LGBM-Classifier-3\",\n",
    "        \"model\": LGBMClassifier, \n",
    "        \"params\": {\"n_estimators\": 100, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\": \"SVC-1\",\n",
    "        \"model\": SVC, \n",
    "        \"params\": {\"C\" : 0.1, \"random_state\": SEED, \"cache_size\": 5000},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\": \"SVC-2\",\n",
    "        \"model\": SVC, \n",
    "        \"params\": {\"C\" : 1, \"random_state\": SEED, \"cache_size\": 5000},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\": \"SVC-3\",\n",
    "        \"model\": SVC, \n",
    "        \"params\": {\"C\" : 10, \"random_state\": SEED, \"cache_size\": 5000},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\": \"RandomForest-Classifier-1\",\n",
    "        \"model\": RandomForestClassifier, \n",
    "        \"params\": {\"n_estimators\": 30, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\": \"RandomForest-Classifier-2\",\n",
    "        \"model\": RandomForestClassifier, \n",
    "        \"params\": {\"n_estimators\": 50, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\": \"RandomForest-Classifier-3\",\n",
    "        \"model\": RandomForestClassifier, \n",
    "        \"params\": {\"n_estimators\": 100, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\": \"LogisticRegression-1\",\n",
    "        \"model\": LogisticRegression, \n",
    "        \"params\": {\"C\" : 0.1, \"random_state\": SEED, \"n_jobs\": 4},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\": \"LogisticRegression-2\",\n",
    "        \"model\": LogisticRegression, \n",
    "        \"params\": {\"C\" : 1, \"random_state\": SEED, \"n_jobs\": -1},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\": \"LogisticRegression-3\",\n",
    "        \"model\": LogisticRegression, \n",
    "        \"params\": {\"C\" : 10, \"random_state\": SEED, \"n_jobs\": -1},\n",
    "    },\n",
    "]\n",
    "    \n",
    "X_train_fold = train_data[train_data[\"Fold\"] != FOLD]\n",
    "X_valid_fold = train_data[train_data[\"Fold\"] == FOLD]\n",
    "\n",
    "y_train = X_train_fold[\"Genre\"]\n",
    "y_valid = X_valid_fold[\"Genre\"]\n",
    "y_test  = test_data[\"Genre\"]\n",
    "\n",
    "valid_predictions = pd.DataFrame()\n",
    "valid_predictions[\"Label\"] = y_valid\n",
    "\n",
    "test_predictions = pd.DataFrame()\n",
    "test_predictions[\"Label\"] = y_test\n",
    "\n",
    "tic = time.time()\n",
    "for config in MODELS:\n",
    "    for n_grams in NGRAM_RANGES:\n",
    "        for analyzer in ANALYZERS:\n",
    "            key = f\"[{config['id']}][{analyzer}][{n_grams}]\"\n",
    "            X_train_features = scipy.sparse.load_npz(os.path.join(PATH_TO_STORAGE, f'fold-{FOLD}', f'train_features_ngrams_{n_grams}_analyzer_{analyzer}.npz'))\n",
    "            X_valid_features = scipy.sparse.load_npz(os.path.join(PATH_TO_STORAGE, f'fold-{FOLD}', f'valid_features_ngrams_{n_grams}_analyzer_{analyzer}.npz'))\n",
    "            X_test_features  = scipy.sparse.load_npz(os.path.join(PATH_TO_STORAGE, f'fold-{FOLD}', f'test_features_ngrams_{n_grams}_analyzer_{analyzer}.npz'))\n",
    "            \n",
    "            model = config[\"model\"](**config[\"params\"])\n",
    "            model.fit(X_train_features, y_train)\n",
    "\n",
    "            y_predict_valid  = model.predict(X_valid_features)\n",
    "            y_predict_test   = model.predict(X_test_features)\n",
    "            \n",
    "            valid_predictions[key] = y_predict_valid \n",
    "            test_predictions[key]  = y_predict_test\n",
    "            \n",
    "            print(f\"{key} Baseline Accuracy: {ROUND(accuracy_score(y_valid, y_predict_valid))}\")\n",
    "    \n",
    "    valid_predictions.to_csv(os.path.join(PATH_TO_PREDICTIONS, \"valid_predictions_stage_0.csv\"), index = False)\n",
    "    test_predictions.to_csv(os.path.join(PATH_TO_PREDICTIONS, \"test_predictions_stage_0.csv\"), index = False)\n",
    "\n",
    "toc = time.time()\n",
    "print(f\"Training Stage 0 Time: {(ROUND(toc - tic))}'s\")\n",
    "if os.path.isdir(PATH_TO_PREDICTIONS) == False: os.makedirs(PATH_TO_PREDICTIONS)\n",
    "\n",
    "valid_predictions.to_csv(os.path.join(PATH_TO_PREDICTIONS, \"valid_predictions_stage_0.csv\"), index = False)\n",
    "test_predictions.to_csv(os.path.join(PATH_TO_PREDICTIONS, \"test_predictions_stage_0.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the previous training stage, we obtained the following results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions = pd.read_csv(os.path.join(PATH_TO_PREDICTIONS, \"valid_predictions_stage_0.csv\"))\n",
    "test_predictions  = pd.read_csv(os.path.join(PATH_TO_PREDICTIONS, \"test_predictions_stage_0.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 10 Models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('[LGBM-Classifier-3][char_wb][(1, 5)]', 0.4351),\n",
       " ('[LGBM-Classifier-3][word][(1, 1)]', 0.4286),\n",
       " ('[LogisticRegression-3][word][(1, 2)]', 0.4278),\n",
       " ('[LGBM-Classifier-3][char_wb][(1, 7)]', 0.427),\n",
       " ('[LGBM-Classifier-3][char_wb][(1, 3)]', 0.4261),\n",
       " ('[LGBM-Classifier-3][word][(1, 3)]', 0.424),\n",
       " ('[LGBM-Classifier-3][word][(1, 5)]', 0.424),\n",
       " ('[SVC-3][word][(1, 1)]', 0.4213),\n",
       " ('[LGBM-Classifier-2][char_wb][(1, 7)]', 0.4199),\n",
       " ('[LogisticRegression-3][word][(1, 3)]', 0.4188)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = []\n",
    "for column in valid_predictions.columns.tolist():\n",
    "    if column == 'Label': continue\n",
    "    models.append((column, ROUND(accuracy_score(valid_predictions['Label'], valid_predictions[column]))))\n",
    "    \n",
    "models = sorted(models, key = lambda accuracy: accuracy[1], reverse = True)\n",
    "print(\"Best 10 Models\")\n",
    "display(models[: 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we expected, some models prefered word prespective other character perspective \n",
    "\n",
    "# Now we need the select best models from our experiments\n",
    "### We will search for a threshold between 40% and 43% accuracy as a condition for the model to enter in the ensemble\n",
    "- This search will be done on the validation step and applied on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy: 0.45989738050229545 at threshold: 0.421\n"
     ]
    }
   ],
   "source": [
    "best_threshold = None \n",
    "best_accuracy  = -np.inf\n",
    "for threshold in np.arange(0.40, 0.43, 0.001):\n",
    "    selected_models = [key for key, accuracy in models if accuracy > threshold]\n",
    "    predictions = valid_predictions[selected_models]\n",
    "    predictions = predictions.values\n",
    "    final_predictions = []\n",
    "    for i in range(predictions.shape[0]):\n",
    "        values, counts = np.unique(predictions[i], return_counts = True)\n",
    "        index = np.argmax(counts)\n",
    "        final_predictions.append(values[index])\n",
    "\n",
    "    valid_predictions['Ensemble'] = final_predictions\n",
    "    accuracy = accuracy_score(valid_predictions['Label'], valid_predictions['Ensemble'])\n",
    "    \n",
    "    if accuracy >= best_accuracy:\n",
    "        best_accuracy  = accuracy\n",
    "        best_threshold = threshold\n",
    "        \n",
    "print(f\"Best Validation Accuracy: {best_accuracy} at threshold: {ROUND(best_threshold)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the threshold and implicitly the models\n",
    "### We need the ensure that our models predictions are as low correlated as possible for increased variance in the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[LGBM-Classifier-3][char_wb][(1, 5)]</th>\n",
       "      <th>[LGBM-Classifier-3][word][(1, 1)]</th>\n",
       "      <th>[LogisticRegression-3][word][(1, 2)]</th>\n",
       "      <th>[LGBM-Classifier-3][char_wb][(1, 7)]</th>\n",
       "      <th>[LGBM-Classifier-3][char_wb][(1, 3)]</th>\n",
       "      <th>[LGBM-Classifier-3][word][(1, 3)]</th>\n",
       "      <th>[LGBM-Classifier-3][word][(1, 5)]</th>\n",
       "      <th>[SVC-3][word][(1, 1)]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[LGBM-Classifier-3][char_wb][(1, 5)]</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.495941</td>\n",
       "      <td>0.422069</td>\n",
       "      <td>0.703573</td>\n",
       "      <td>0.656552</td>\n",
       "      <td>0.484277</td>\n",
       "      <td>0.451558</td>\n",
       "      <td>0.422062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[LGBM-Classifier-3][word][(1, 1)]</th>\n",
       "      <td>0.495941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459893</td>\n",
       "      <td>0.498205</td>\n",
       "      <td>0.461701</td>\n",
       "      <td>0.616393</td>\n",
       "      <td>0.564790</td>\n",
       "      <td>0.481691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[LogisticRegression-3][word][(1, 2)]</th>\n",
       "      <td>0.422069</td>\n",
       "      <td>0.459893</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420390</td>\n",
       "      <td>0.421915</td>\n",
       "      <td>0.475927</td>\n",
       "      <td>0.475776</td>\n",
       "      <td>0.650679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[LGBM-Classifier-3][char_wb][(1, 7)]</th>\n",
       "      <td>0.703573</td>\n",
       "      <td>0.498205</td>\n",
       "      <td>0.420390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.656727</td>\n",
       "      <td>0.458192</td>\n",
       "      <td>0.455082</td>\n",
       "      <td>0.421316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[LGBM-Classifier-3][char_wb][(1, 3)]</th>\n",
       "      <td>0.656552</td>\n",
       "      <td>0.461701</td>\n",
       "      <td>0.421915</td>\n",
       "      <td>0.656727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.436391</td>\n",
       "      <td>0.434041</td>\n",
       "      <td>0.420420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[LGBM-Classifier-3][word][(1, 3)]</th>\n",
       "      <td>0.484277</td>\n",
       "      <td>0.616393</td>\n",
       "      <td>0.475927</td>\n",
       "      <td>0.458192</td>\n",
       "      <td>0.436391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.694717</td>\n",
       "      <td>0.454785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[LGBM-Classifier-3][word][(1, 5)]</th>\n",
       "      <td>0.451558</td>\n",
       "      <td>0.564790</td>\n",
       "      <td>0.475776</td>\n",
       "      <td>0.455082</td>\n",
       "      <td>0.434041</td>\n",
       "      <td>0.694717</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.437203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[SVC-3][word][(1, 1)]</th>\n",
       "      <td>0.422062</td>\n",
       "      <td>0.481691</td>\n",
       "      <td>0.650679</td>\n",
       "      <td>0.421316</td>\n",
       "      <td>0.420420</td>\n",
       "      <td>0.454785</td>\n",
       "      <td>0.437203</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      [LGBM-Classifier-3][char_wb][(1, 5)]  \\\n",
       "[LGBM-Classifier-3][char_wb][(1, 5)]                              1.000000   \n",
       "[LGBM-Classifier-3][word][(1, 1)]                                 0.495941   \n",
       "[LogisticRegression-3][word][(1, 2)]                              0.422069   \n",
       "[LGBM-Classifier-3][char_wb][(1, 7)]                              0.703573   \n",
       "[LGBM-Classifier-3][char_wb][(1, 3)]                              0.656552   \n",
       "[LGBM-Classifier-3][word][(1, 3)]                                 0.484277   \n",
       "[LGBM-Classifier-3][word][(1, 5)]                                 0.451558   \n",
       "[SVC-3][word][(1, 1)]                                             0.422062   \n",
       "\n",
       "                                      [LGBM-Classifier-3][word][(1, 1)]  \\\n",
       "[LGBM-Classifier-3][char_wb][(1, 5)]                           0.495941   \n",
       "[LGBM-Classifier-3][word][(1, 1)]                              1.000000   \n",
       "[LogisticRegression-3][word][(1, 2)]                           0.459893   \n",
       "[LGBM-Classifier-3][char_wb][(1, 7)]                           0.498205   \n",
       "[LGBM-Classifier-3][char_wb][(1, 3)]                           0.461701   \n",
       "[LGBM-Classifier-3][word][(1, 3)]                              0.616393   \n",
       "[LGBM-Classifier-3][word][(1, 5)]                              0.564790   \n",
       "[SVC-3][word][(1, 1)]                                          0.481691   \n",
       "\n",
       "                                      [LogisticRegression-3][word][(1, 2)]  \\\n",
       "[LGBM-Classifier-3][char_wb][(1, 5)]                              0.422069   \n",
       "[LGBM-Classifier-3][word][(1, 1)]                                 0.459893   \n",
       "[LogisticRegression-3][word][(1, 2)]                              1.000000   \n",
       "[LGBM-Classifier-3][char_wb][(1, 7)]                              0.420390   \n",
       "[LGBM-Classifier-3][char_wb][(1, 3)]                              0.421915   \n",
       "[LGBM-Classifier-3][word][(1, 3)]                                 0.475927   \n",
       "[LGBM-Classifier-3][word][(1, 5)]                                 0.475776   \n",
       "[SVC-3][word][(1, 1)]                                             0.650679   \n",
       "\n",
       "                                      [LGBM-Classifier-3][char_wb][(1, 7)]  \\\n",
       "[LGBM-Classifier-3][char_wb][(1, 5)]                              0.703573   \n",
       "[LGBM-Classifier-3][word][(1, 1)]                                 0.498205   \n",
       "[LogisticRegression-3][word][(1, 2)]                              0.420390   \n",
       "[LGBM-Classifier-3][char_wb][(1, 7)]                              1.000000   \n",
       "[LGBM-Classifier-3][char_wb][(1, 3)]                              0.656727   \n",
       "[LGBM-Classifier-3][word][(1, 3)]                                 0.458192   \n",
       "[LGBM-Classifier-3][word][(1, 5)]                                 0.455082   \n",
       "[SVC-3][word][(1, 1)]                                             0.421316   \n",
       "\n",
       "                                      [LGBM-Classifier-3][char_wb][(1, 3)]  \\\n",
       "[LGBM-Classifier-3][char_wb][(1, 5)]                              0.656552   \n",
       "[LGBM-Classifier-3][word][(1, 1)]                                 0.461701   \n",
       "[LogisticRegression-3][word][(1, 2)]                              0.421915   \n",
       "[LGBM-Classifier-3][char_wb][(1, 7)]                              0.656727   \n",
       "[LGBM-Classifier-3][char_wb][(1, 3)]                              1.000000   \n",
       "[LGBM-Classifier-3][word][(1, 3)]                                 0.436391   \n",
       "[LGBM-Classifier-3][word][(1, 5)]                                 0.434041   \n",
       "[SVC-3][word][(1, 1)]                                             0.420420   \n",
       "\n",
       "                                      [LGBM-Classifier-3][word][(1, 3)]  \\\n",
       "[LGBM-Classifier-3][char_wb][(1, 5)]                           0.484277   \n",
       "[LGBM-Classifier-3][word][(1, 1)]                              0.616393   \n",
       "[LogisticRegression-3][word][(1, 2)]                           0.475927   \n",
       "[LGBM-Classifier-3][char_wb][(1, 7)]                           0.458192   \n",
       "[LGBM-Classifier-3][char_wb][(1, 3)]                           0.436391   \n",
       "[LGBM-Classifier-3][word][(1, 3)]                              1.000000   \n",
       "[LGBM-Classifier-3][word][(1, 5)]                              0.694717   \n",
       "[SVC-3][word][(1, 1)]                                          0.454785   \n",
       "\n",
       "                                      [LGBM-Classifier-3][word][(1, 5)]  \\\n",
       "[LGBM-Classifier-3][char_wb][(1, 5)]                           0.451558   \n",
       "[LGBM-Classifier-3][word][(1, 1)]                              0.564790   \n",
       "[LogisticRegression-3][word][(1, 2)]                           0.475776   \n",
       "[LGBM-Classifier-3][char_wb][(1, 7)]                           0.455082   \n",
       "[LGBM-Classifier-3][char_wb][(1, 3)]                           0.434041   \n",
       "[LGBM-Classifier-3][word][(1, 3)]                              0.694717   \n",
       "[LGBM-Classifier-3][word][(1, 5)]                              1.000000   \n",
       "[SVC-3][word][(1, 1)]                                          0.437203   \n",
       "\n",
       "                                      [SVC-3][word][(1, 1)]  \n",
       "[LGBM-Classifier-3][char_wb][(1, 5)]               0.422062  \n",
       "[LGBM-Classifier-3][word][(1, 1)]                  0.481691  \n",
       "[LogisticRegression-3][word][(1, 2)]               0.650679  \n",
       "[LGBM-Classifier-3][char_wb][(1, 7)]               0.421316  \n",
       "[LGBM-Classifier-3][char_wb][(1, 3)]               0.420420  \n",
       "[LGBM-Classifier-3][word][(1, 3)]                  0.454785  \n",
       "[LGBM-Classifier-3][word][(1, 5)]                  0.437203  \n",
       "[SVC-3][word][(1, 1)]                              1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_models = [key for key, accuracy in models if accuracy > best_threshold]\n",
    "predictions = valid_predictions[selected_models]\n",
    "display(predictions.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, there are no strong correlated predictions so we can continue without excluding any model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Test Accuracy based on current progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Accuracy: 0.45989738050229545\n"
     ]
    }
   ],
   "source": [
    "predictions = predictions.values\n",
    "final_predictions = []\n",
    "for i in range(predictions.shape[0]):\n",
    "    values, counts = np.unique(predictions[i], return_counts = True)\n",
    "    index = np.argmax(counts)\n",
    "    final_predictions.append(values[index])\n",
    "\n",
    "valid_predictions['Ensemble'] = final_predictions\n",
    "print(\"Current Validation Accuracy: {}\".format(accuracy_score(valid_predictions['Label'], valid_predictions['Ensemble'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Test Accuracy: 0.45267800882167614\n"
     ]
    }
   ],
   "source": [
    "predictions = test_predictions[selected_models].values\n",
    "\n",
    "final_predictions = []\n",
    "for i in range(predictions.shape[0]):\n",
    "    values, counts = np.unique(predictions[i], return_counts = True)\n",
    "    index = np.argmax(counts)\n",
    "    final_predictions.append(values[index])\n",
    "\n",
    "test_predictions['Ensemble'] = final_predictions\n",
    "print(\"Current Test Accuracy: {}\".format(accuracy_score(test_predictions['Label'], test_predictions['Ensemble'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretty decent results on validation and test set\n",
    "### Without any hyper tuning of the parameters\n",
    "### Our best model obtains 43.5% accuracy on the validation set and our voting system obtains 46% accuracy\n",
    "### And also test and validation results are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[LGBM-Classifier-3][char_wb][(1, 5)]',\n",
       " '[LGBM-Classifier-3][word][(1, 1)]',\n",
       " '[LogisticRegression-3][word][(1, 2)]',\n",
       " '[LGBM-Classifier-3][char_wb][(1, 7)]',\n",
       " '[LGBM-Classifier-3][char_wb][(1, 3)]',\n",
       " '[LGBM-Classifier-3][word][(1, 3)]',\n",
       " '[LGBM-Classifier-3][word][(1, 5)]',\n",
       " '[SVC-3][word][(1, 1)]']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(selected_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usually this would be the hyper-parameter search stage\n",
    "- For the selected models with preprocessing contexts, we should find the optimal hyper-parameters based on random search or bayesian optimization\n",
    "- Because of time constraints, we will only try to tickle the main hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    {\n",
    "        \"name\": \"LGBMClassifier\", \"model\": LGBMClassifier, \"analyzer\": \"char_wb\",\"n_grams\": (1, 5),\n",
    "        \"parameters\": [\n",
    "            {\"n_estimators\": 100, \"random_state\": SEED, \"n_jobs\": - 1}, {\"n_estimators\": 150, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "            {\"n_estimators\": 200, \"random_state\": SEED, \"n_jobs\": - 1}, {\"n_estimators\": 300, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "            {\"n_estimators\": 500, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"LGBMClassifier\", \"model\": LGBMClassifier, \"analyzer\": \"word\", \"n_grams\": (1, 1),\n",
    "        \"parameters\": [\n",
    "            {\"n_estimators\": 100, \"random_state\": SEED, \"n_jobs\": - 1}, {\"n_estimators\": 150, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "            {\"n_estimators\": 200, \"random_state\": SEED, \"n_jobs\": - 1}, {\"n_estimators\": 300, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "            {\"n_estimators\": 500, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"LogisticRegression\", \"model\": LogisticRegression, \"analyzer\": \"word\", \"n_grams\": (1, 2),\n",
    "        \"parameters\": [\n",
    "            {\"C\" : 7,  \"random_state\": SEED, \"n_jobs\": -1}, {\"C\" : 10, \"random_state\": SEED, \"n_jobs\": -1},\n",
    "            {\"C\" : 12, \"random_state\": SEED, \"n_jobs\": -1}, {\"C\" : 15, \"random_state\": SEED, \"n_jobs\": -1},\n",
    "            {\"C\" : 20, \"random_state\": SEED, \"n_jobs\": -1},\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"LGBMClassifier\", \"model\": LGBMClassifier, \"analyzer\": \"char_wb\", \"n_grams\": (1, 7),\n",
    "        \"parameters\": [\n",
    "            {\"n_estimators\": 100, \"random_state\": SEED, \"n_jobs\": - 1}, {\"n_estimators\": 150, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "            {\"n_estimators\": 200, \"random_state\": SEED, \"n_jobs\": - 1}, {\"n_estimators\": 300, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "            {\"n_estimators\": 500, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"LGBMClassifier\", \"model\": LGBMClassifier, \"analyzer\": \"char_wb\", \"n_grams\": (1, 3),\n",
    "        \"parameters\": [\n",
    "            {\"n_estimators\": 100, \"random_state\": SEED, \"n_jobs\": - 1}, {\"n_estimators\": 150, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "            {\"n_estimators\": 200, \"random_state\": SEED, \"n_jobs\": - 1}, {\"n_estimators\": 300, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "            {\"n_estimators\": 500, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"LGBMClassifier\", \"model\": LGBMClassifier, \"analyzer\": \"word\", \"n_grams\": (1, 3),\n",
    "        \"parameters\": [\n",
    "            {\"n_estimators\": 100, \"random_state\": SEED, \"n_jobs\": - 1}, {\"n_estimators\": 150, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "            {\"n_estimators\": 200, \"random_state\": SEED, \"n_jobs\": - 1}, {\"n_estimators\": 300, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "            {\"n_estimators\": 500, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"LGBMClassifier\", \"model\": LGBMClassifier, \"analyzer\": \"word\", \"n_grams\": (1, 5),\n",
    "        \"parameters\": [\n",
    "            {\"n_estimators\": 100, \"random_state\": SEED, \"n_jobs\": - 1}, {\"n_estimators\": 150, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "            {\"n_estimators\": 200, \"random_state\": SEED, \"n_jobs\": - 1}, {\"n_estimators\": 300, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "            {\"n_estimators\": 500, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"SVC\", \"model\": SVC, \"analyzer\": \"word\", \"n_grams\": (1, 1),\n",
    "        \"parameters\": [\n",
    "            {\"C\" : 7,  \"random_state\": SEED}, {\"C\" : 10, \"random_state\": SEED},\n",
    "            {\"C\" : 12, \"random_state\": SEED}, {\"C\" : 15, \"random_state\": SEED}, {\"C\" : 20, \"random_state\": SEED},\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0\n",
    "X_train_fold = train_data[train_data[\"Fold\"] != FOLD]\n",
    "X_valid_fold = train_data[train_data[\"Fold\"] == FOLD]\n",
    "\n",
    "y_train = X_train_fold[\"Genre\"]\n",
    "y_valid = X_valid_fold[\"Genre\"]\n",
    "y_test  = test_data[\"Genre\"]\n",
    "\n",
    "valid_predictions = pd.DataFrame()\n",
    "valid_predictions[\"Label\"] = y_valid\n",
    "\n",
    "test_predictions = pd.DataFrame()\n",
    "test_predictions[\"Label\"] = y_test\n",
    "\n",
    "tic = time.time()\n",
    "for config in MODELS:\n",
    "    key = f\"[{config['name']}][{config['analyzer']}][{config['n_grams']}]\"\n",
    "    X_train_features = scipy.sparse.load_npz(os.path.join(PATH_TO_STORAGE, f'fold-{FOLD}', f'train_features_ngrams_{config[\"n_grams\"]}_analyzer_{config[\"analyzer\"]}.npz'))\n",
    "    X_valid_features = scipy.sparse.load_npz(os.path.join(PATH_TO_STORAGE, f'fold-{FOLD}', f'valid_features_ngrams_{config[\"n_grams\"]}_analyzer_{config[\"analyzer\"]}.npz'))\n",
    "    X_test_features  = scipy.sparse.load_npz(os.path.join(PATH_TO_STORAGE, f'fold-{FOLD}', f'test_features_ngrams_{config[\"n_grams\"]}_analyzer_{config[\"analyzer\"]}.npz'))\n",
    "\n",
    "    for step, param in enumerate(config[\"parameters\"]):\n",
    "        second_key = key + f\"[param-{step}]\"\n",
    "        model = config[\"model\"](**param)\n",
    "        model.fit(X_train_features, y_train)\n",
    "\n",
    "        y_predict_valid  = model.predict(X_valid_features)\n",
    "        y_predict_test   = model.predict(X_test_features)\n",
    "\n",
    "        valid_predictions[second_key] = y_predict_valid \n",
    "        test_predictions[second_key]  = y_predict_test\n",
    "\n",
    "        print(f\"{key} Baseline Accuracy: {ROUND(accuracy_score(y_valid, y_predict_valid))}\")\n",
    "        valid_predictions.to_csv(os.path.join(PATH_TO_PREDICTIONS, \"valid_predictions_stage_1.csv\"), index = False)\n",
    "        test_predictions.to_csv(os.path.join(PATH_TO_PREDICTIONS, \"test_predictions_stage_1.csv\"), index = False)\n",
    "\n",
    "toc = time.time()\n",
    "print(f\"Training Stage 1 Time: {(ROUND(toc - tic))}'s\")\n",
    "if os.path.isdir(PATH_TO_PREDICTIONS) == False: os.makedirs(PATH_TO_PREDICTIONS)\n",
    "\n",
    "valid_predictions.to_csv(os.path.join(PATH_TO_PREDICTIONS, \"valid_predictions_stage_1.csv\"), index = False)\n",
    "test_predictions.to_csv(os.path.join(PATH_TO_PREDICTIONS, \"test_predictions_stage_1.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After this hyper-parameter search stage, we obtain the following results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions = pd.read_csv(os.path.join(PATH_TO_PREDICTIONS, \"final_valid_predictions_stage_1.csv\"))\n",
    "test_predictions  = pd.read_csv(os.path.join(PATH_TO_PREDICTIONS, \"final_test_predictions_stage_1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 10 Models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('[LGBMClassifier][char_wb][(1, 5)][param-1]', 0.441),\n",
       " ('[LGBMClassifier][char_wb][(1, 5)][param-2]', 0.4402),\n",
       " ('[LGBMClassifier][char_wb][(1, 5)][param-4]', 0.4399),\n",
       " ('[LGBMClassifier][char_wb][(1, 7)][param-4]', 0.4383),\n",
       " ('[LGBMClassifier][char_wb][(1, 7)][param-1]', 0.4369),\n",
       " ('[LGBMClassifier][char_wb][(1, 7)][param-2]', 0.4367),\n",
       " ('[LGBMClassifier][char_wb][(1, 5)][param-3]', 0.4361),\n",
       " ('[LGBMClassifier][char_wb][(1, 7)][param-3]', 0.4356),\n",
       " ('[LGBMClassifier][char_wb][(1, 5)][param-0]', 0.4351),\n",
       " ('[LGBMClassifier][char_wb][(1, 3)][param-3]', 0.434)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = []\n",
    "for column in valid_predictions.columns.tolist():\n",
    "    if column == 'Label': continue\n",
    "    models.append((column, ROUND(accuracy_score(valid_predictions['Label'], valid_predictions[column]))))\n",
    "    \n",
    "models = sorted(models, key = lambda accuracy: accuracy[1], reverse = True)\n",
    "print(\"Best 10 Models\")\n",
    "display(models[: 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the unique models with best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[LGBMClassifier][char_wb][(1, 5)][param-1]',\n",
       " '[LGBMClassifier][char_wb][(1, 7)][param-4]',\n",
       " '[LGBMClassifier][char_wb][(1, 3)][param-3]',\n",
       " '[LGBMClassifier][word][(1, 1)][param-1]',\n",
       " '[LGBMClassifier][word][(1, 3)][param-1]',\n",
       " '[LogisticRegression][word][(1, 2)][param-1]',\n",
       " '[LGBMClassifier][word][(1, 5)][param-2]',\n",
       " '[SVC][word][(1, 1)][param-0]']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_names   = []\n",
    "unique_models  = []\n",
    "unique_columns = []\n",
    "\n",
    "for name, accuracy in models:\n",
    "    unpacked_key = \"/\".join(\"\".join(name.split(\"[\")).split(\"]\")[:3])\n",
    "    param_id     = \"\".join(name.split(\"[\")).split(\"]\")[3]\n",
    "\n",
    "    if unpacked_key not in unique_names:\n",
    "        unique_names.append(unpacked_key)\n",
    "        unique_models.append((unpacked_key + \"/\" + param_id, accuracy))\n",
    "\n",
    "for unpacked_key, _ in unique_models:\n",
    "    model, analyzer, n_grams, param_id = unpacked_key.split(\"/\")\n",
    "    key = f\"[{model}][{analyzer}][{n_grams}][{param_id}]\"\n",
    "    unique_columns.append(key)\n",
    "    \n",
    "display(unique_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Test Accuracy based on current progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Accuracy: 0.462597893599784\n"
     ]
    }
   ],
   "source": [
    "predictions = valid_predictions[unique_columns]\n",
    "predictions = predictions.values\n",
    "final_predictions = []\n",
    "for i in range(predictions.shape[0]):\n",
    "    values, counts = np.unique(predictions[i], return_counts = True)\n",
    "    index = np.argmax(counts)\n",
    "    final_predictions.append(values[index])\n",
    "\n",
    "valid_predictions['Ensemble'] = final_predictions\n",
    "print(\"Current Validation Accuracy: {}\".format(accuracy_score(valid_predictions['Label'], valid_predictions['Ensemble'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Test Accuracy: 0.45784499054820416\n"
     ]
    }
   ],
   "source": [
    "predictions = test_predictions[unique_columns].values\n",
    "\n",
    "final_predictions = []\n",
    "for i in range(predictions.shape[0]):\n",
    "    values, counts = np.unique(predictions[i], return_counts = True)\n",
    "    index = np.argmax(counts)\n",
    "    final_predictions.append(values[index])\n",
    "\n",
    "test_predictions['Ensemble'] = final_predictions\n",
    "print(\"Current Test Accuracy: {}\".format(accuracy_score(test_predictions['Label'], test_predictions['Ensemble'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, with the best models and hyper-parameters we can compute the OOF (Out Of Fold) Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    {\n",
    "        \"name\": \"LGBMClassifier\", \"model\": LGBMClassifier, \"analyzer\": \"char_wb\", \"n_grams\": (1, 5),\n",
    "        \"parameters\": {\"n_estimators\": 150, \"random_state\": SEED, \"n_jobs\": - 1}\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"LGBMClassifier\", \"model\": LGBMClassifier, \"analyzer\": \"char_wb\", \"n_grams\": (1, 7),\n",
    "        \"parameters\": {\"n_estimators\": 500, \"random_state\": SEED, \"n_jobs\": - 1}\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"LGBMClassifier\", \"model\": LGBMClassifier, \"analyzer\": \"char_wb\", \"n_grams\": (1, 3),\n",
    "        \"parameters\": {\"n_estimators\": 300, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "    },\n",
    "        \n",
    "    {\n",
    "        \"name\": \"LGBMClassifier\", \"model\": LGBMClassifier, \"analyzer\": \"word\", \"n_grams\": (1, 1),\n",
    "        \"parameters\": {\"n_estimators\": 150, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"LGBMClassifier\", \"model\": LGBMClassifier, \"analyzer\": \"char_wb\", \"n_grams\": (1, 3),\n",
    "        \"parameters\": {\"n_estimators\": 150, \"random_state\": SEED, \"n_jobs\": - 1},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"LogisticRegression\", \"model\": LogisticRegression, \"analyzer\": \"word\", \"n_grams\": (1, 2),\n",
    "        \"parameters\": {\"C\" : 10, \"random_state\": SEED, \"n_jobs\": -1},\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"LGBMClassifier\", \"model\": LGBMClassifier, \"analyzer\": \"word\", \"n_grams\": (1, 5),\n",
    "        \"parameters\": {\"n_estimators\": 200, \"random_state\": SEED, \"n_jobs\": - 1}, \n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"SVC\", \"model\": SVC, \"analyzer\": \"word\", \"n_grams\": (1, 1),\n",
    "        \"parameters\": {\"C\" : 7,  \"random_state\": SEED}, \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Training Results\n",
    "- Each fold will predicti the test set resulting in $M\\_MODELS * N\\_FOLDS$ predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LGBMClassifier][char_wb][(1, 5)] Fold: 0 Baseline Accuracy: 0.441\n",
      "[LGBMClassifier][char_wb][(1, 5)] Fold: 1 Baseline Accuracy: 0.4518\n",
      "[LGBMClassifier][char_wb][(1, 5)] Fold: 2 Baseline Accuracy: 0.4418\n",
      "[LGBMClassifier][char_wb][(1, 5)] Fold: 3 Baseline Accuracy: 0.4322\n",
      "[LGBMClassifier][char_wb][(1, 5)] Fold: 4 Baseline Accuracy: 0.4317\n",
      "[LGBMClassifier][char_wb][(1, 7)] Fold: 0 Baseline Accuracy: 0.4383\n",
      "[LGBMClassifier][char_wb][(1, 7)] Fold: 1 Baseline Accuracy: 0.4429\n",
      "[LGBMClassifier][char_wb][(1, 7)] Fold: 2 Baseline Accuracy: 0.4423\n",
      "[LGBMClassifier][char_wb][(1, 7)] Fold: 3 Baseline Accuracy: 0.4406\n",
      "[LGBMClassifier][char_wb][(1, 7)] Fold: 4 Baseline Accuracy: 0.4352\n",
      "[LGBMClassifier][char_wb][(1, 3)] Fold: 0 Baseline Accuracy: 0.434\n",
      "[LGBMClassifier][char_wb][(1, 3)] Fold: 1 Baseline Accuracy: 0.434\n",
      "[LGBMClassifier][char_wb][(1, 3)] Fold: 2 Baseline Accuracy: 0.4321\n",
      "[LGBMClassifier][char_wb][(1, 3)] Fold: 3 Baseline Accuracy: 0.4257\n",
      "[LGBMClassifier][char_wb][(1, 3)] Fold: 4 Baseline Accuracy: 0.4249\n",
      "[LGBMClassifier][word][(1, 1)] Fold: 0 Baseline Accuracy: 0.4324\n",
      "[LGBMClassifier][word][(1, 1)] Fold: 1 Baseline Accuracy: 0.421\n",
      "[LGBMClassifier][word][(1, 1)] Fold: 2 Baseline Accuracy: 0.4248\n",
      "[LGBMClassifier][word][(1, 1)] Fold: 3 Baseline Accuracy: 0.4241\n",
      "[LGBMClassifier][word][(1, 1)] Fold: 4 Baseline Accuracy: 0.4198\n",
      "[LGBMClassifier][char_wb][(1, 3)] Fold: 0 Baseline Accuracy: 0.4302\n",
      "[LGBMClassifier][char_wb][(1, 3)] Fold: 1 Baseline Accuracy: 0.4418\n",
      "[LGBMClassifier][char_wb][(1, 3)] Fold: 2 Baseline Accuracy: 0.4318\n",
      "[LGBMClassifier][char_wb][(1, 3)] Fold: 3 Baseline Accuracy: 0.42\n",
      "[LGBMClassifier][char_wb][(1, 3)] Fold: 4 Baseline Accuracy: 0.4284\n",
      "[LogisticRegression][word][(1, 2)] Fold: 0 Baseline Accuracy: 0.4278\n",
      "[LogisticRegression][word][(1, 2)] Fold: 1 Baseline Accuracy: 0.421\n",
      "[LogisticRegression][word][(1, 2)] Fold: 2 Baseline Accuracy: 0.4232\n",
      "[LogisticRegression][word][(1, 2)] Fold: 3 Baseline Accuracy: 0.4103\n",
      "[LogisticRegression][word][(1, 2)] Fold: 4 Baseline Accuracy: 0.4173\n",
      "[LGBMClassifier][word][(1, 5)] Fold: 0 Baseline Accuracy: 0.4272\n",
      "[LGBMClassifier][word][(1, 5)] Fold: 1 Baseline Accuracy: 0.4245\n",
      "[LGBMClassifier][word][(1, 5)] Fold: 2 Baseline Accuracy: 0.4224\n",
      "[LGBMClassifier][word][(1, 5)] Fold: 3 Baseline Accuracy: 0.4082\n",
      "[LGBMClassifier][word][(1, 5)] Fold: 4 Baseline Accuracy: 0.4157\n",
      "[SVC][word][(1, 1)] Fold: 0 Baseline Accuracy: 0.4216\n",
      "[SVC][word][(1, 1)] Fold: 1 Baseline Accuracy: 0.4175\n",
      "[SVC][word][(1, 1)] Fold: 2 Baseline Accuracy: 0.4186\n",
      "[SVC][word][(1, 1)] Fold: 3 Baseline Accuracy: 0.4103\n",
      "[SVC][word][(1, 1)] Fold: 4 Baseline Accuracy: 0.4044\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "test_predictions = pd.DataFrame()\n",
    "oof = np.zeros((len(train_data), len(MODELS)))\n",
    "\n",
    "for model_idx, config in enumerate(MODELS):\n",
    "    key = f\"[{config['name']}][{config['analyzer']}][{config['n_grams']}]\"\n",
    "    \n",
    "    for fold in range(FOLDS):\n",
    "        valid_idx  = train_data[train_data[\"Fold\"] == fold].index\n",
    "        second_key = key + f\"[Fold-{fold}]\"\n",
    "        \n",
    "        X_train_fold = train_data[train_data[\"Fold\"] != fold]\n",
    "        X_valid_fold = train_data[train_data[\"Fold\"] == fold]\n",
    "        \n",
    "        y_train = X_train_fold[\"Genre\"]\n",
    "        y_valid = X_valid_fold[\"Genre\"]\n",
    "        y_test  = test_data[\"Genre\"]\n",
    "\n",
    "        X_train_features = scipy.sparse.load_npz(os.path.join(PATH_TO_STORAGE, f'fold-{fold}', f'train_features_ngrams_{config[\"n_grams\"]}_analyzer_{config[\"analyzer\"]}.npz'))\n",
    "        X_valid_features = scipy.sparse.load_npz(os.path.join(PATH_TO_STORAGE, f'fold-{fold}', f'valid_features_ngrams_{config[\"n_grams\"]}_analyzer_{config[\"analyzer\"]}.npz'))\n",
    "        X_test_features  = scipy.sparse.load_npz(os.path.join(PATH_TO_STORAGE, f'fold-{fold}', f'test_features_ngrams_{config[\"n_grams\"]}_analyzer_{config[\"analyzer\"]}.npz'))\n",
    "\n",
    "        model = config[\"model\"](**config[\"parameters\"])\n",
    "        model.fit(X_train_features, y_train)\n",
    "\n",
    "        y_predict_valid  = model.predict(X_valid_features)\n",
    "        y_predict_test   = model.predict(X_test_features)\n",
    "\n",
    "        test_predictions[second_key] = y_predict_test\n",
    "        oof[valid_idx, model_idx]    = y_predict_valid\n",
    "                                                              \n",
    "        print(f\"{key} Fold: {fold} Baseline Accuracy: {ROUND(accuracy_score(y_valid, y_predict_valid))}\")      \n",
    "        \n",
    "    pd.DataFrame(oof).to_csv(\"./predictions/oof.csv\", index = False)\n",
    "    test_predictions.to_csv(\"./predictions/test_predictions.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0: OOF Accuracy: 0.43969102792632203\n",
      "Model 1: OOF Accuracy: 0.4398530762167126\n",
      "Model 2: OOF Accuracy: 0.4301301787932804\n",
      "Model 3: OOF Accuracy: 0.42440447253281477\n",
      "Model 4: OOF Accuracy: 0.43045427537406145\n",
      "Model 5: OOF Accuracy: 0.41992113649867663\n",
      "Model 6: OOF Accuracy: 0.41959703991789554\n",
      "Model 7: OOF Accuracy: 0.41446551072219523\n",
      "OOF Ensemble Accuracy: 0.45724625938529684\n"
     ]
    }
   ],
   "source": [
    "out_of_fold_predictions          = pd.read_csv(\"./predictions/oof.csv\")\n",
    "out_of_fold_predictions[\"Fold\"]  = train_data[\"Fold\"]\n",
    "out_of_fold_predictions['Label'] = train_data[\"Genre\"]\n",
    "\n",
    "for i in range(8): print(\"Model {}: OOF Accuracy: {}\".format(i, accuracy_score(oof[str(i)], out_of_fold_predictions[\"Label\"])))\n",
    "    \n",
    "predictions = out_of_fold_predictions[[str(i) for i in range(8)]]\n",
    "predictions = predictions.values\n",
    "final_predictions = []\n",
    "for i in range(predictions.shape[0]):\n",
    "    values, counts = np.unique(predictions[i], return_counts = True)\n",
    "    index = np.argmax(counts)\n",
    "    final_predictions.append(values[index])\n",
    "\n",
    "out_of_fold_predictions['Ensemble'] = final_predictions\n",
    "print(\"OOF Ensemble Accuracy: {}\".format(accuracy_score(out_of_fold_predictions['Label'], out_of_fold_predictions['Ensemble'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Methods instead of Voting System for combining models\n",
    "- Unfortunately, stacking needs more hyper-parameter tuning so in our case did not improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 Baseline Accuracy: 0.4618\n",
      "Fold: 1 Baseline Accuracy: 0.454\n",
      "Fold: 2 Baseline Accuracy: 0.4677\n",
      "Fold: 3 Baseline Accuracy: 0.4449\n",
      "Fold: 4 Baseline Accuracy: 0.4495\n",
      "Stacking Predictions Accuracy: 0.4555717603845946\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "stacking = np.zeros((len(train_data),))\n",
    "for fold in range(FOLDS):\n",
    "    valid_idx = out_of_fold_predictions[out_of_fold_predictions[\"Fold\"] == fold].index\n",
    "    \n",
    "    X_train_fold = out_of_fold_predictions[out_of_fold_predictions[\"Fold\"] != fold]\n",
    "    X_valid_fold = out_of_fold_predictions[out_of_fold_predictions[\"Fold\"] == fold]\n",
    "\n",
    "    y_train = X_train_fold[\"Label\"]\n",
    "    y_valid = X_valid_fold[\"Label\"]\n",
    "    \n",
    "    X_train = X_train_fold[[str(i) for i in range(8)]]\n",
    "    X_valid = X_valid_fold[[str(i) for i in range(8)]]\n",
    "    \n",
    "    model = XGBClassifier(random_state = SEED, n_jobs = -1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_predict_valid  = model.predict(X_valid)\n",
    "    stacking[valid_idx] = y_predict_valid\n",
    "    \n",
    "    print(f\"Fold: {fold} Baseline Accuracy: {ROUND(accuracy_score(y_valid, y_predict_valid))}\")   \n",
    "    \n",
    "print(\"Stacking Predictions Accuracy: {}\".format(accuracy_score(out_of_fold_predictions[\"Label\"], stacking)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Statistic Stage] Final Test Accuracy: 0.465658475110271\n"
     ]
    }
   ],
   "source": [
    "oof_test_predictions = pd.read_csv(\"./predictions/test_predictions.csv\")\n",
    "oof_models = oof_test_predictions.columns.tolist()\n",
    "oof_test_predictions[\"Label\"] = test_data['Genre']\n",
    "\n",
    "predictions = oof_test_predictions[oof_models]\n",
    "predictions = predictions.values\n",
    "final_predictions = []\n",
    "for i in range(predictions.shape[0]):\n",
    "    values, counts = np.unique(predictions[i], return_counts = True)\n",
    "    index = np.argmax(counts)\n",
    "    final_predictions.append(values[index])\n",
    "\n",
    "oof_test_predictions['Ensemble'] = final_predictions\n",
    "print(\"[Statistic Stage] Final Test Accuracy: {}\".format(accuracy_score(oof_test_predictions['Label'], oof_test_predictions['Ensemble'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Stage 2: Training Transformers (RoBERTa, XLNet)\n",
    "- For training those models we used a 2080Ti with 11Gb using SimpleTransformers Library\n",
    "- Code for the script attached in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import os\n",
    "    import torch\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    def seed_everything(seed = 42):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    SEED = 42\n",
    "    seed_everything(seed = SEED)\n",
    "\n",
    "    train_data = pd.read_csv(\"data/Lyrics-Genre-Train.csv\") \n",
    "    test_data  = pd.read_csv(\"data/Lyrics-Genre-Test-GroundTruth.csv\")\n",
    "\n",
    "    train_data.drop([\"Song\", \"Song year\", \"Artist\", \"Track_id\"], axis = 1, inplace = True)\n",
    "    test_data.drop([\"Song\", \"Song year\", \"Artist\", \"Track_id\"],  axis = 1, inplace = True)\n",
    "\n",
    "    encoder = OrdinalEncoder()\n",
    "    encoder.fit(train_data['Genre'].values.reshape(-1, 1))\n",
    "    train_data['Genre'] = encoder.transform(train_data['Genre'].values.reshape(-1, 1)).astype(np.uint8)\n",
    "    test_data['Genre']  = encoder.transform(test_data['Genre'].values.reshape(-1, 1)).astype(np.uint8)\n",
    "\n",
    "    train_data = train_data.rename(columns = {\"Lyrics\": \"text\", \"Genre\": \"label\"})\n",
    "    test_data  = test_data.rename(columns = {\"Lyrics\": \"text\", \"Genre\": \"label\"})\n",
    "\n",
    "    train_data          = train_data[['text', 'label']]\n",
    "    train_data['text']  = train_data['text'].astype(str)\n",
    "    train_data['label'] = train_data['label'].astype(int)\n",
    "\n",
    "    test_data          = test_data[['text', 'label']]\n",
    "    test_data['text']  = test_data['text'].astype(str)\n",
    "    test_data['label'] = test_data['label'].astype(int)\n",
    "\n",
    "    args = {\n",
    "        \"output_dir\": \"outputs/\",\n",
    "        \"cache_dir\": \"cache_dir/\",\n",
    "\n",
    "        \"fp16\": True,\n",
    "        \"fp16_opt_level\": \"O1\",\n",
    "        \"max_seq_length\": 128,\n",
    "        \"train_batch_size\": 48,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"eval_batch_size\": 48,\n",
    "        \"num_train_epochs\": 10,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 7e-6,\n",
    "\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"warmup_ratio\": 0.06,\n",
    "        \"warmup_steps\": 0,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "\n",
    "        \"logging_steps\": 50,\n",
    "        \"save_steps\": 2000,\n",
    "\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"reprocess_input_data\": True,\n",
    "        \"evaluate_during_training\": True,\n",
    "        \"evaluate_during_training_silent\": False,\n",
    "\n",
    "        \"n_gpu\": 1,\n",
    "\n",
    "        \"do_lower_case\": False,\n",
    "        \"early_stopping_consider_epochs\": True,\n",
    "        \"early_stopping_metric\": \"accuracy\",\n",
    "        \"early_stopping_metric_minimize\": False,\n",
    "        \"early_stopping_patience\": 1,\n",
    "        \"use_early_stopping\": True,\n",
    "    }\n",
    "\n",
    "    model_name = 'roberta-base'\n",
    "    model = ClassificationModel('roberta', model_name, num_labels = 10, args = args)\n",
    "\n",
    "    model.train_model(train_data, eval_df = test_data, accuracy = lambda truth, predictions: accuracy_score(\n",
    "            truth, [round(p) for p in predictions]\n",
    "        ))\n",
    "\n",
    "    result, model_outputs, wrong_predictions = model.eval_model(test_data, acc = accuracy_score)\n",
    "\n",
    "    #print(torch.tensor(model_outputs).shape)\n",
    "\n",
    "    print(result)\n",
    "    if result['acc'] > 0.45:\n",
    "        preds = pd.DataFrame()\n",
    "        preds[model_name] = torch.argmax(torch.nn.functional.softmax(torch.tensor(model_outputs), dim = 1), dim = 1)\n",
    "        preds.to_csv(model_name + \"-\" + str(np.round(result['acc'], 4)) + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This resulted in the following models predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Semantic Stage] Final Test Accuracy Score: 0.4657844990548204\n"
     ]
    }
   ],
   "source": [
    "transformers = pd.DataFrame()\n",
    "server = [\n",
    "    'server/roberta-base-0.461.csv',\n",
    "    'server/roberta-base-0.4531.csv',\n",
    "    'server/roberta-base-0.4534.csv',\n",
    "    'server/roberta-base-0.4582.csv',\n",
    "    'server/roberta-base-0.4599.csv',\n",
    "    'server/xlnet-base-cased-0.4543.csv',\n",
    "    'server/xlnet-base-cased-0.4573.csv',\n",
    "]\n",
    "\n",
    "\n",
    "for model, path in enumerate(server):\n",
    "    data = pd.read_csv(path)\n",
    "    if \"roberta\" in data.columns.tolist()[0]:\n",
    "        transformers[f\"model-{model}-roberta-base\"] = data[\"roberta-base\"]\n",
    "    else:\n",
    "        transformers[f\"model-{model}-xlnet-base-cased\"] = data[\"xlnet-base-cased\"]\n",
    "        \n",
    "predictions = transformers[transformers.columns.tolist()]\n",
    "predictions = predictions.values\n",
    "\n",
    "final_predictions = []\n",
    "for i in range(predictions.shape[0]):\n",
    "    values, counts = np.unique(predictions[i], return_counts = True)\n",
    "    index = np.argmax(counts)\n",
    "    final_predictions.append(values[index])\n",
    "\n",
    "transformers['Ensemble'] = final_predictions\n",
    "transformers['Label']    = test_data[\"Genre\"]\n",
    "\n",
    "print(\"[Semantic Stage] Final Test Accuracy Score: {}\".format(accuracy_score(transformers['Ensemble'], transformers['Label'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting System between Statistic Models and Semantic Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[LGBMClassifier][char_wb][(1, 5)][Fold-0]</th>\n",
       "      <th>[LGBMClassifier][char_wb][(1, 5)][Fold-1]</th>\n",
       "      <th>[LGBMClassifier][char_wb][(1, 5)][Fold-2]</th>\n",
       "      <th>[LGBMClassifier][char_wb][(1, 5)][Fold-3]</th>\n",
       "      <th>[LGBMClassifier][char_wb][(1, 5)][Fold-4]</th>\n",
       "      <th>[LGBMClassifier][char_wb][(1, 7)][Fold-0]</th>\n",
       "      <th>[LGBMClassifier][char_wb][(1, 7)][Fold-1]</th>\n",
       "      <th>[LGBMClassifier][char_wb][(1, 7)][Fold-2]</th>\n",
       "      <th>[LGBMClassifier][char_wb][(1, 7)][Fold-3]</th>\n",
       "      <th>[LGBMClassifier][char_wb][(1, 7)][Fold-4]</th>\n",
       "      <th>...</th>\n",
       "      <th>[SVC][word][(1, 1)][Fold-4]</th>\n",
       "      <th>model-0-roberta-base</th>\n",
       "      <th>model-1-roberta-base</th>\n",
       "      <th>model-2-roberta-base</th>\n",
       "      <th>model-3-roberta-base</th>\n",
       "      <th>model-4-roberta-base</th>\n",
       "      <th>model-5-xlnet-base-cased</th>\n",
       "      <th>model-6-xlnet-base-cased</th>\n",
       "      <th>Ensemble</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7930</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7931</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7932</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7933</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7935 rows  44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      [LGBMClassifier][char_wb][(1, 5)][Fold-0]  \\\n",
       "0                                             9   \n",
       "1                                             9   \n",
       "2                                             9   \n",
       "3                                             7   \n",
       "4                                             7   \n",
       "...                                         ...   \n",
       "7930                                          0   \n",
       "7931                                          6   \n",
       "7932                                          3   \n",
       "7933                                          9   \n",
       "7934                                          4   \n",
       "\n",
       "      [LGBMClassifier][char_wb][(1, 5)][Fold-1]  \\\n",
       "0                                             9   \n",
       "1                                             9   \n",
       "2                                             9   \n",
       "3                                             9   \n",
       "4                                             7   \n",
       "...                                         ...   \n",
       "7930                                          0   \n",
       "7931                                          6   \n",
       "7932                                          3   \n",
       "7933                                          4   \n",
       "7934                                          4   \n",
       "\n",
       "      [LGBMClassifier][char_wb][(1, 5)][Fold-2]  \\\n",
       "0                                             3   \n",
       "1                                             9   \n",
       "2                                             9   \n",
       "3                                             7   \n",
       "4                                             5   \n",
       "...                                         ...   \n",
       "7930                                          0   \n",
       "7931                                          6   \n",
       "7932                                          3   \n",
       "7933                                          4   \n",
       "7934                                          9   \n",
       "\n",
       "      [LGBMClassifier][char_wb][(1, 5)][Fold-3]  \\\n",
       "0                                             3   \n",
       "1                                             7   \n",
       "2                                             9   \n",
       "3                                             7   \n",
       "4                                             7   \n",
       "...                                         ...   \n",
       "7930                                          0   \n",
       "7931                                          6   \n",
       "7932                                          3   \n",
       "7933                                          9   \n",
       "7934                                          9   \n",
       "\n",
       "      [LGBMClassifier][char_wb][(1, 5)][Fold-4]  \\\n",
       "0                                             3   \n",
       "1                                             9   \n",
       "2                                             9   \n",
       "3                                             7   \n",
       "4                                             5   \n",
       "...                                         ...   \n",
       "7930                                          0   \n",
       "7931                                          6   \n",
       "7932                                          3   \n",
       "7933                                          9   \n",
       "7934                                          4   \n",
       "\n",
       "      [LGBMClassifier][char_wb][(1, 7)][Fold-0]  \\\n",
       "0                                             3   \n",
       "1                                             9   \n",
       "2                                             9   \n",
       "3                                             9   \n",
       "4                                             9   \n",
       "...                                         ...   \n",
       "7930                                          9   \n",
       "7931                                          6   \n",
       "7932                                          3   \n",
       "7933                                          9   \n",
       "7934                                          4   \n",
       "\n",
       "      [LGBMClassifier][char_wb][(1, 7)][Fold-1]  \\\n",
       "0                                             3   \n",
       "1                                             9   \n",
       "2                                             9   \n",
       "3                                             7   \n",
       "4                                             7   \n",
       "...                                         ...   \n",
       "7930                                          0   \n",
       "7931                                          6   \n",
       "7932                                          3   \n",
       "7933                                          4   \n",
       "7934                                          9   \n",
       "\n",
       "      [LGBMClassifier][char_wb][(1, 7)][Fold-2]  \\\n",
       "0                                             3   \n",
       "1                                             9   \n",
       "2                                             9   \n",
       "3                                             7   \n",
       "4                                             7   \n",
       "...                                         ...   \n",
       "7930                                          0   \n",
       "7931                                          6   \n",
       "7932                                          3   \n",
       "7933                                          9   \n",
       "7934                                          9   \n",
       "\n",
       "      [LGBMClassifier][char_wb][(1, 7)][Fold-3]  \\\n",
       "0                                             3   \n",
       "1                                             7   \n",
       "2                                             9   \n",
       "3                                             7   \n",
       "4                                             5   \n",
       "...                                         ...   \n",
       "7930                                          9   \n",
       "7931                                          6   \n",
       "7932                                          3   \n",
       "7933                                          4   \n",
       "7934                                          9   \n",
       "\n",
       "      [LGBMClassifier][char_wb][(1, 7)][Fold-4]  ...  \\\n",
       "0                                             3  ...   \n",
       "1                                             4  ...   \n",
       "2                                             9  ...   \n",
       "3                                             7  ...   \n",
       "4                                             7  ...   \n",
       "...                                         ...  ...   \n",
       "7930                                          7  ...   \n",
       "7931                                          6  ...   \n",
       "7932                                          3  ...   \n",
       "7933                                          9  ...   \n",
       "7934                                          4  ...   \n",
       "\n",
       "      [SVC][word][(1, 1)][Fold-4]  model-0-roberta-base  model-1-roberta-base  \\\n",
       "0                               3                     0                     2   \n",
       "1                               9                     9                     9   \n",
       "2                               9                     9                     6   \n",
       "3                               7                     7                     7   \n",
       "4                               5                     5                     5   \n",
       "...                           ...                   ...                   ...   \n",
       "7930                            0                     9                     9   \n",
       "7931                            6                     6                     6   \n",
       "7932                            3                     3                     3   \n",
       "7933                            9                     9                     9   \n",
       "7934                            4                     4                     9   \n",
       "\n",
       "      model-2-roberta-base  model-3-roberta-base  model-4-roberta-base  \\\n",
       "0                        0                     0                     0   \n",
       "1                        9                     9                     9   \n",
       "2                        9                     6                     9   \n",
       "3                        7                     7                     7   \n",
       "4                        5                     5                     0   \n",
       "...                    ...                   ...                   ...   \n",
       "7930                     7                     9                     9   \n",
       "7931                     6                     6                     6   \n",
       "7932                     3                     3                     3   \n",
       "7933                     9                     9                     9   \n",
       "7934                     9                     9                     9   \n",
       "\n",
       "      model-5-xlnet-base-cased  model-6-xlnet-base-cased  Ensemble  Label  \n",
       "0                            0                         9         0      3  \n",
       "1                            9                         9         9      4  \n",
       "2                            8                         9         9      6  \n",
       "3                            7                         7         7      7  \n",
       "4                            5                         5         5      3  \n",
       "...                        ...                       ...       ...    ...  \n",
       "7930                         7                         7         9      9  \n",
       "7931                         6                         6         6      6  \n",
       "7932                         3                         3         3      3  \n",
       "7933                         9                         9         9      9  \n",
       "7934                         9                         9         9      9  \n",
       "\n",
       "[7935 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Statistic, Semantic Stage] Final Test Accuracy: 0.4763705103969754\n"
     ]
    }
   ],
   "source": [
    "test_predictions = pd.read_csv(\"./predictions/test_predictions.csv\")\n",
    "models = test_predictions.columns.tolist()\n",
    "\n",
    "for transformer_model in transformers.columns.tolist():\n",
    "    test_predictions[transformer_model] = transformers[transformer_model]\n",
    "\n",
    "display(test_predictions)\n",
    "\n",
    "predictions = test_predictions[[col for col in test_predictions.columns.tolist() if col != \"Label\" and col != \"Ensemble\"]]\n",
    "predictions = predictions.values\n",
    "\n",
    "final_predictions = []\n",
    "for i in range(predictions.shape[0]):\n",
    "    values, counts = np.unique(predictions[i], return_counts = True)\n",
    "    index = np.argmax(counts)\n",
    "    final_predictions.append(values[index])\n",
    "\n",
    "test_predictions['Ensemble'] = final_predictions\n",
    "print(\"[Statistic, Semantic Stage] Final Test Accuracy: {}\".format(accuracy_score(test_predictions['Label'], test_predictions['Ensemble'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Statistic features at word and character level with transformers obtained 47.6%\n",
    "\n",
    "- Combining models trained on statistic features at word and character level for different ngrams with transformers obtained the highest accuracy for this project \n",
    "- Unfortunately, using embeddings for lyrics similarity did not improve the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following section I will add generating embeddings, training and prediction stage for lyrics similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Embeddings for Lyrics Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import os\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    from IPython.display import display\n",
    "\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    SEED = 42\n",
    "\n",
    "    def seed_everything(seed = 42):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    seed_everything(seed = SEED)\n",
    "\n",
    "    train_data = pd.read_csv(\"data/Lyrics-Genre-Train.csv\") \n",
    "    test_data  = pd.read_csv(\"data/Lyrics-Genre-Test-GroundTruth.csv\")\n",
    "\n",
    "    train_data.drop([\"Song\", \"Song year\", \"Artist\"], axis = 1, inplace = True)\n",
    "    test_data.drop([\"Song\", \"Song year\", \"Artist\"], axis = 1, inplace = True)\n",
    "\n",
    "    encoder = OrdinalEncoder()\n",
    "    encoder.fit(train_data['Genre'].values.reshape(-1, 1))\n",
    "    train_data['Genre'] = encoder.transform(train_data['Genre'].values.reshape(-1, 1)).astype(np.uint8)\n",
    "    test_data['Genre']  = encoder.transform(test_data['Genre'].values.reshape(-1, 1)).astype(np.uint8)\n",
    "\n",
    "    # display(train_data.head(n = 3))\n",
    "    # display(train_data.shape)\n",
    "\n",
    "    # display(test_data.head(n = 3))\n",
    "    # display(test_data.shape)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = SEED)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(train_data, train_data['Genre'].values)):\n",
    "        train_data.loc[valid_idx, 'Fold'] = fold\n",
    "\n",
    "    train_data['Fold'] = train_data['Fold'].astype(int)    \n",
    "\n",
    "    # ! pip install transformers\n",
    "    # ! pip install sentence-transformers\n",
    "\n",
    "    model = SentenceTransformer('stsb-roberta-large')\n",
    "\n",
    "    embeddings = pd.DataFrame(columns = [\"Track_id\", \"Fold\", \"Genre\"] + [f\"Feature_{i}\" for i in range(1024)])\n",
    "    for row, sample in test_data.iterrows():\n",
    "        track_id = sample['Track_id']\n",
    "        lyrics   = sample['Lyrics']\n",
    "        label    = sample['Genre']\n",
    "\n",
    "        embedding = model.encode(lyrics, convert_to_tensor = True).detach().cpu().numpy().tolist()\n",
    "        row = [track_id, label, fold] + embedding\n",
    "        embeddings.loc[len(embeddings)] = row\n",
    "\n",
    "        # break\n",
    "\n",
    "    embeddings.to_csv(\"train_embeddings.csv\", index = False)\n",
    "    display(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train Embeddings Shape: (18513, 1027)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test Embeddings Shape: (7935, 1026)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_embeddings = pd.read_csv('data/train_embeddings.csv')\n",
    "test_embeddings  = pd.read_csv('data/test_embeddings.csv')\n",
    "\n",
    "display(\"Train Embeddings Shape: {}\".format(train_embeddings.shape))\n",
    "display(\"Test Embeddings Shape: {}\".format(test_embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Similarity Embeddings] [Fold 0] Baseline Accuracy: 0.3575479341074804\n",
      "[Similarity Embeddings] [Fold 1] Baseline Accuracy: 0.34620577909802863\n",
      "[Similarity Embeddings] [Fold 2] Baseline Accuracy: 0.35565757493923844\n",
      "[Similarity Embeddings] [Fold 3] Baseline Accuracy: 0.33684494867639114\n",
      "[Similarity Embeddings] [Fold 4] Baseline Accuracy: 0.3419773095623987\n"
     ]
    }
   ],
   "source": [
    "features = [f\"Feature_{i}\" for i in range(1024)]\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    X_train_emeddings = train_embeddings[train_embeddings['Fold'] != fold][features]\n",
    "    X_valid_emeddings = train_embeddings[train_embeddings['Fold'] == fold][features]\n",
    "\n",
    "    y_train_emeddings = train_embeddings[train_embeddings['Fold'] != fold][\"Genre\"]\n",
    "    y_valid_emeddings = train_embeddings[train_embeddings['Fold'] == fold][\"Genre\"]\n",
    "    \n",
    "    model = LGBMClassifier(random_state = SEED, n_jobs = -1)\n",
    "    model.fit(X_train_emeddings, y_train_emeddings)\n",
    "    \n",
    "    y_preds_embeddings = model.predict(X_valid_emeddings)\n",
    "    print(f\"[Similarity Embeddings] [Fold {fold}] Baseline Accuracy: {accuracy_score(y_preds_embeddings, y_valid_emeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even if we need as much variance as possible in our voting ensemble, we can't use a model with a score as low as ~34% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thing that did not work\n",
    "- Using pretrained text similarity embeddings for lyrics classification (check section above)\n",
    "- Removing Stop Words before TF-IDF Features\n",
    "- Lowercasing before TF-IDF Features\n",
    "- Training Bert/DistilBERT Transformer Models\n",
    "\n",
    "# Ideas worth trying\n",
    "- Training Transformer Model embedding and using them SVC classification head\n",
    "- Finding ways for text cleaning without losing information\n",
    "- Extracting styling features, training models and adding them to ensabmble\n",
    "- Using more intervals for ngrams with TF-IDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
